{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T18:14:24.610900Z",
     "start_time": "2019-06-07T18:14:24.396790Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T18:14:29.596950Z",
     "start_time": "2019-06-07T18:14:24.939694Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DE-348\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample input:\n",
    "\n",
    "```\n",
    "{\"msg\": {\"id\": 0, \"name\": \"test 0\"}, \"attr\": {\"timestamp\": 1555705314, \"tf\": true, \"details\": [{\"a\": 0, \"b\":100}, {\"a\": 10, \"b\": 1000}]}}\n",
    "{\"msg\": {\"id\": 1, \"name\": \"test 1\"}, \"attr\": {\"timestamp\": 1555715314, \"tf\": true, \"details\": [{\"a\": 1, \"b\":101}, {\"a\": 11, \"b\": 1001}]}}\n",
    "{\"msg\": {\"id\": 2, \"name\": \"test 2\"}, \"attr\": {\"timestamp\": 1555725314, \"tf\": true, \"details\": [{\"a\": 2, \"b\":102}, {\"a\": 12, \"b\": 1002}]}}\n",
    "{\"msg\": {\"id\": 3, \"name\": \"test 3\"}, \"attr\": {\"timestamp\": 1555735314, \"tf\": false, \"details\": []}}\n",
    "{\"msg\": {\"id\": 4, \"name\": \"test 4\"}, \"attr\": {\"timestamp\": 1555745314, \"tf\": true, \"details\": [{\"a\": 4, \"b\":104}, {\"a\": 14, \"b\": 1004}]}}\n",
    "{\"msg\": {\"id\": 5, \"name\": \"test 5\"}, \"attr\": {\"timestamp\": 1555755314, \"tf\": true, \"details\": [{\"a\": 5, \"b\":105}, {\"a\": 15, \"b\": 1005}]}}\n",
    "{\"msg\": {\"id\": 6, \"name\": \"test 6\"}, \"attr\": {\"timestamp\": 1555765314, \"tf\": false, \"details\": []}}\n",
    "{\"msg\": {\"id\": 7, \"name\": \"test 7\"}, \"attr\": {\"timestamp\": 1555775314, \"tf\": true, \"details\": [{\"a\": 7, \"b\":107}, {\"a\": 17, \"b\": 1007}]}}\n",
    "{\"msg\": {\"id\": 8, \"name\": \"test 8\"}, \"attr\": {\"timestamp\": 1555785314, \"tf\": true, \"details\": [{\"a\": 8, \"b\":108}, {\"a\": 18, \"b\": 1008}]}}\n",
    "{\"msg\": {\"id\": 9, \"name\": \"test 9\"}, \"attr\": {\"timestamp\": 1555795314, \"tf\": false, \"details\": []}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T18:14:35.385614Z",
     "start_time": "2019-06-07T18:14:31.394276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                attr|        msg|\n",
      "+--------------------+-----------+\n",
      "|[[[0, 100], [10, ...|[0, test 0]|\n",
      "|[[[1, 101], [11, ...|[1, test 1]|\n",
      "|[[[2, 102], [12, ...|[2, test 2]|\n",
      "|[[], false, 15557...|[3, test 3]|\n",
      "|[[[4, 104], [14, ...|[4, test 4]|\n",
      "|[[[5, 105], [15, ...|[5, test 5]|\n",
      "|[[], false, 15557...|[6, test 6]|\n",
      "|[[[7, 107], [17, ...|[7, test 7]|\n",
      "|[[[8, 108], [18, ...|[8, test 8]|\n",
      "|[[], false, 15557...|[9, test 9]|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF = spark.read.json(\"test_input1.json\")\n",
    "testDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T18:14:35.434632Z",
     "start_time": "2019-06-07T18:14:35.387617Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as pysql_fn\n",
    "import pyspark.sql.types as pysql_types\n",
    "\n",
    "def flatten_structs(df):\n",
    "    \"\"\"\n",
    "    This function will flatten all StructTypes from a pyspark dataframe recursively.\n",
    "    For example, if each record contains key/value pairs such as {message: {...}, data: {...}}, these are flattened as:\n",
    "    {message-field1, message-field2, .... message-fieldx, data-field1, ... data-fieldx}\n",
    "    However, more complex types such as arrays are not handled. Therefore, an array of structs remains as is\n",
    "    :param df: a Spark dataframe\n",
    "    :return: a flattened Spark dataframe\n",
    "    \"\"\"\n",
    "    # Get a list of columns that are StructType\n",
    "    # df.schema.fields returns [StructField(name, dataType, nullable)]\n",
    "    f_struct_type_cols = filter(lambda field: isinstance(field.dataType, pysql_types.StructType), df.schema.fields)\n",
    "    struct_type_cols = list(f_struct_type_cols)\n",
    "\n",
    "    if len(struct_type_cols) == 0:\n",
    "        return df\n",
    "\n",
    "    unnested_struct_cols = [{\n",
    "        'name': field.name,\n",
    "        'fields': [inner_field.name for inner_field in field.dataType.fields]\n",
    "    } for field in struct_type_cols]\n",
    "\n",
    "    # Remove the StructType columns from the list of all columns of the DF\n",
    "    non_struct_cols = [\n",
    "        pysql_fn.col(col_name) for col_name in df.schema.names if col_name not in {\n",
    "            struct_col.name for struct_col in struct_type_cols\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    select_cols = []\n",
    "    for col_dict in unnested_struct_cols:\n",
    "        for col_field in col_dict['fields']:\n",
    "            col_name = \"{}.{}\".format(col_dict['name'], col_field)\n",
    "            col_name_alias = \"{}-{}\".format(col_dict['name'], col_field)\n",
    "            select_cols.append(pysql_fn.col(col_name).alias(col_name_alias))\n",
    "\n",
    "    unnested_df = df.select(non_struct_cols + select_cols)\n",
    "\n",
    "    return flatten_structs(unnested_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T18:14:39.350062Z",
     "start_time": "2019-06-07T18:14:39.122635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------+------+--------+\n",
      "|        attr-details|attr-tf|attr-timestamp|msg-id|msg-name|\n",
      "+--------------------+-------+--------------+------+--------+\n",
      "|[[0, 100], [10, 1...|   true|    1555705314|     0|  test 0|\n",
      "|[[1, 101], [11, 1...|   true|    1555715314|     1|  test 1|\n",
      "|[[2, 102], [12, 1...|   true|    1555725314|     2|  test 2|\n",
      "|                  []|  false|    1555735314|     3|  test 3|\n",
      "|[[4, 104], [14, 1...|   true|    1555745314|     4|  test 4|\n",
      "|[[5, 105], [15, 1...|   true|    1555755314|     5|  test 5|\n",
      "|                  []|  false|    1555765314|     6|  test 6|\n",
      "|[[7, 107], [17, 1...|   true|    1555775314|     7|  test 7|\n",
      "|[[8, 108], [18, 1...|   true|    1555785314|     8|  test 8|\n",
      "|                  []|  false|    1555795314|     9|  test 9|\n",
      "+--------------------+-------+--------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatTestDF = flatten_structs(testDF)\n",
    "flatTestDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T18:14:46.578788Z",
     "start_time": "2019-06-07T18:14:46.354976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------+------+--------+--------------------+\n",
      "|        attr-details|attr-tf|attr-timestamp|msg-id|msg-name|               js_ad|\n",
      "+--------------------+-------+--------------+------+--------+--------------------+\n",
      "|[[0, 100], [10, 1...|   true|    1555705314|     0|  test 0|[{\"a\":0,\"b\":100},...|\n",
      "|[[1, 101], [11, 1...|   true|    1555715314|     1|  test 1|[{\"a\":1,\"b\":101},...|\n",
      "|[[2, 102], [12, 1...|   true|    1555725314|     2|  test 2|[{\"a\":2,\"b\":102},...|\n",
      "|                  []|  false|    1555735314|     3|  test 3|                  []|\n",
      "|[[4, 104], [14, 1...|   true|    1555745314|     4|  test 4|[{\"a\":4,\"b\":104},...|\n",
      "|[[5, 105], [15, 1...|   true|    1555755314|     5|  test 5|[{\"a\":5,\"b\":105},...|\n",
      "|                  []|  false|    1555765314|     6|  test 6|                  []|\n",
      "|[[7, 107], [17, 1...|   true|    1555775314|     7|  test 7|[{\"a\":7,\"b\":107},...|\n",
      "|[[8, 108], [18, 1...|   true|    1555785314|     8|  test 8|[{\"a\":8,\"b\":108},...|\n",
      "|                  []|  false|    1555795314|     9|  test 9|                  []|\n",
      "+--------------------+-------+--------------+------+--------+--------------------+\n",
      "\n",
      "root\n",
      " |-- attr-details: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- a: long (nullable = true)\n",
      " |    |    |-- b: long (nullable = true)\n",
      " |-- attr-tf: boolean (nullable = true)\n",
      " |-- attr-timestamp: long (nullable = true)\n",
      " |-- msg-id: long (nullable = true)\n",
      " |-- msg-name: string (nullable = true)\n",
      " |-- js_ad: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsTestDF = flatTestDF.withColumn(\"js_ad\", pysql_fn.to_json(pysql_fn.col(\"attr-details\")))\n",
    "jsTestDF.show()\n",
    "jsTestDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `interimDF` below will be roughly what the data will look like coming from the interim data bucket. We will not have the `attr-details` column from above, but rather the json string encoded version as seen in `js_ad`. Ultimately this is achieved with the pyspark function `to_json`.\n",
    "\n",
    "While ideally we would then be able to read this column back into the correct format using the `from_json` function, this function requires specifying a schema, which we will not have. Therefore, I convert the `js_ad` column to a parseable json row as an RDD and then read it into a new dataframe using `spark.read.json`. I can then `explode` it to extract the details and join it to the column I need from the `interimDF` to produce what will finally be the `finalDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T18:14:50.657470Z",
     "start_time": "2019-06-07T18:14:50.539124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------+--------+--------------------+\n",
      "|attr-tf|attr-timestamp|msg-id|msg-name|               js_ad|\n",
      "+-------+--------------+------+--------+--------------------+\n",
      "|   true|    1555705314|     0|  test 0|[{\"a\":0,\"b\":100},...|\n",
      "|   true|    1555715314|     1|  test 1|[{\"a\":1,\"b\":101},...|\n",
      "|   true|    1555725314|     2|  test 2|[{\"a\":2,\"b\":102},...|\n",
      "|  false|    1555735314|     3|  test 3|                  []|\n",
      "|   true|    1555745314|     4|  test 4|[{\"a\":4,\"b\":104},...|\n",
      "|   true|    1555755314|     5|  test 5|[{\"a\":5,\"b\":105},...|\n",
      "|  false|    1555765314|     6|  test 6|                  []|\n",
      "|   true|    1555775314|     7|  test 7|[{\"a\":7,\"b\":107},...|\n",
      "|   true|    1555785314|     8|  test 8|[{\"a\":8,\"b\":108},...|\n",
      "|  false|    1555795314|     9|  test 9|                  []|\n",
      "+-------+--------------+------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interimDF = jsTestDF.drop(\"attr-details\")\n",
    "interimDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T19:01:17.033087Z",
     "start_time": "2019-06-07T19:01:16.923911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|             details|msg-id|\n",
      "+--------------------+------+\n",
      "|[[0, 100], [10, 1...|     0|\n",
      "|[[1, 101], [11, 1...|     1|\n",
      "|[[2, 102], [12, 1...|     2|\n",
      "|                  []|     3|\n",
      "|[[4, 104], [14, 1...|     4|\n",
      "|[[5, 105], [15, 1...|     5|\n",
      "|                  []|     6|\n",
      "|[[7, 107], [17, 1...|     7|\n",
      "|[[8, 108], [18, 1...|     8|\n",
      "|                  []|     9|\n",
      "+--------------------+------+\n",
      "\n",
      "root\n",
      " |-- details: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- a: long (nullable = true)\n",
      " |    |    |-- b: long (nullable = true)\n",
      " |-- msg-id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsIntDF = spark.read.json(interimDF.rdd.map(lambda row: {\"msg-id\": row['msg-id'], \"details\": json.loads(row['js_ad'])}))\n",
    "jsIntDF.show()\n",
    "jsIntDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T18:23:00.147671Z",
     "start_time": "2019-06-07T18:23:00.036147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|msg-id|    detail|\n",
      "+------+----------+\n",
      "|     0|  [0, 100]|\n",
      "|     0|[10, 1000]|\n",
      "|     1|  [1, 101]|\n",
      "|     1|[11, 1001]|\n",
      "|     2|  [2, 102]|\n",
      "|     2|[12, 1002]|\n",
      "|     4|  [4, 104]|\n",
      "|     4|[14, 1004]|\n",
      "|     5|  [5, 105]|\n",
      "|     5|[15, 1005]|\n",
      "|     7|  [7, 107]|\n",
      "|     7|[17, 1007]|\n",
      "|     8|  [8, 108]|\n",
      "|     8|[18, 1008]|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expIntDF = jsIntDF.select(\"msg-id\", pysql_fn.explode(\"details\").alias(\"detail\"))\n",
    "expIntDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T14:57:31.906623Z",
     "start_time": "2019-06-07T14:57:31.805528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+\n",
      "|msg-id|  a|   b|\n",
      "+------+---+----+\n",
      "|     0|  0| 100|\n",
      "|     0| 10|1000|\n",
      "|     1|  1| 101|\n",
      "|     1| 11|1001|\n",
      "|     2|  2| 102|\n",
      "|     2| 12|1002|\n",
      "|     4|  4| 104|\n",
      "|     4| 14|1004|\n",
      "|     5|  5| 105|\n",
      "|     5| 15|1005|\n",
      "|     7|  7| 107|\n",
      "|     7| 17|1007|\n",
      "|     8|  8| 108|\n",
      "|     8| 18|1008|\n",
      "+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expFlatIntDF = expIntDF.select(\"msg-id\", \"detail.a\", \"detail.b\")\n",
    "expFlatIntDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T14:58:55.758291Z",
     "start_time": "2019-06-07T14:58:54.540187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------------+-------+----+----+\n",
      "|msg-id|msg-name|attr-timestamp|attr-tf|   a|   b|\n",
      "+------+--------+--------------+-------+----+----+\n",
      "|     0|  test 0|    1555705314|   true|   0| 100|\n",
      "|     0|  test 0|    1555705314|   true|  10|1000|\n",
      "|     1|  test 1|    1555715314|   true|   1| 101|\n",
      "|     1|  test 1|    1555715314|   true|  11|1001|\n",
      "|     2|  test 2|    1555725314|   true|   2| 102|\n",
      "|     2|  test 2|    1555725314|   true|  12|1002|\n",
      "|     3|  test 3|    1555735314|  false|null|null|\n",
      "|     4|  test 4|    1555745314|   true|   4| 104|\n",
      "|     4|  test 4|    1555745314|   true|  14|1004|\n",
      "|     5|  test 5|    1555755314|   true|   5| 105|\n",
      "|     5|  test 5|    1555755314|   true|  15|1005|\n",
      "|     6|  test 6|    1555765314|  false|null|null|\n",
      "|     7|  test 7|    1555775314|   true|   7| 107|\n",
      "|     7|  test 7|    1555775314|   true|  17|1007|\n",
      "|     8|  test 8|    1555785314|   true|   8| 108|\n",
      "|     8|  test 8|    1555785314|   true|  18|1008|\n",
      "|     9|  test 9|    1555795314|  false|null|null|\n",
      "+------+--------+--------------+-------+----+----+\n",
      "\n",
      "root\n",
      " |-- msg-id: long (nullable = true)\n",
      " |-- msg-name: string (nullable = true)\n",
      " |-- attr-timestamp: long (nullable = true)\n",
      " |-- attr-tf: boolean (nullable = true)\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalDF = interimDF.select(\"msg-id\", \"msg-name\", \"attr-timestamp\", \"attr-tf\").join(expFlatIntDF, [\"msg-id\"], how=\"left_outer\")\n",
    "finalDF.orderBy(\"msg-id\").show()\n",
    "finalDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
